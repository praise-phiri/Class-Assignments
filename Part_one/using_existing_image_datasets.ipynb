{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9o2hV6s3JXLHpTUM27BwZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praise-phiri/Class-Assignments/blob/main/Part_one/using_existing_image_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILIZING EXISTING IMAGE DATASETS\n",
        "In the following assignment, we will download an already made dataset, preprocess it, and make it ready for machine learnig models. We will export our dataset in a coco format and we will use the `json` libray in this case. More information about the application of the processes at the end of this notebook"
      ],
      "metadata": {
        "id": "cf9VcWmjz6yF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Dowloading the dataset\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bqM3EK8V2upM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Create directory to store the dataset\n",
        "os.makedirs(\"celeba_dataset\", exist_ok=True)\n",
        "\n",
        "# Download CelebA dataset\n",
        "url = \"https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip\"\n",
        "file_path = \"celeba_dataset/celeba.zip\"\n",
        "urllib.request.urlretrieve(url, file_path)\n",
        "\n",
        "# Extract the downloaded zip file\n",
        "import zipfile\n",
        "with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"celeba_dataset\")\n",
        "\n",
        "# Define paths for images and annotations\n",
        "image_dir = \"celeba_dataset/img_align_celeba/\"\n"
      ],
      "metadata": {
        "id": "rmOO6elH2q9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Preprocess the images and create labels\n"
      ],
      "metadata": {
        "id": "92Gw4arP27XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Read images and create labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for img_file in os.listdir(image_dir)[:30]:  # Taking first 30 images for demonstration\n",
        "    img_path = os.path.join(image_dir, img_file)\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "    images.append(img)\n",
        "    labels.append(img_file.split(\".\")[0])  # Assuming filenames are unique identifiers\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n"
      ],
      "metadata": {
        "id": "_UL265vw3B0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Normalization\n",
        "\n",
        "We have used two feature engineering techniques normalization and data augmentation because of the dataset we have acquired."
      ],
      "metadata": {
        "id": "YrUXeBuy3HXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization\n",
        "images_normalized = images.astype('float32') / 255.0\n"
      ],
      "metadata": {
        "id": "ohXVfbvA3dPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Data Augmentantion"
      ],
      "metadata": {
        "id": "i4ZBuD_73lej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2,\n",
        "                             height_shift_range=0.2, shear_range=0.2,\n",
        "                             zoom_range=0.2, horizontal_flip=True,\n",
        "                             fill_mode='nearest')\n",
        "\n",
        "augmented_images = []\n",
        "for img in images_normalized:\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    augmented_img = datagen.flow(img, batch_size=1).next()[0]\n",
        "    augmented_images.append(augmented_img)\n",
        "\n",
        "augmented_images = np.array(augmented_images)\n"
      ],
      "metadata": {
        "id": "f06Dx8Ww31zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Store Everything in Dataset format\n",
        "In this case, we have used the json library."
      ],
      "metadata": {
        "id": "tQLCqMeV4Uyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels.dtype)"
      ],
      "metadata": {
        "id": "qoe-x9ng-TVY",
        "outputId": "f998b466-3382-4cc0-d3c7-11d58c03973c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<U6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.array(labels)"
      ],
      "metadata": {
        "id": "vi4VNCFs-oQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = labels.astype(str)"
      ],
      "metadata": {
        "id": "o37tkkq5-WaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Let us export the dataset in coco format\n",
        "\n",
        "import json\n",
        "\n",
        "# Create a dictionary to store the COCO format data\n",
        "coco_data = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": [\n",
        "        {\n",
        "            \"id\": 1,\n",
        "            \"name\": \"face\",\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Process images and create image entries\n",
        "image_id = 0\n",
        "for image in augmented_images:\n",
        "    image_entry = {\n",
        "        \"id\": image_id,\n",
        "        \"width\": image.shape[1],\n",
        "        \"height\": image.shape[0],\n",
        "        \"file_name\": f\"image_{image_id}.jpg\",\n",
        "    }\n",
        "    coco_data[\"images\"].append(image_entry)\n",
        "    image_id += 1\n",
        "\n",
        "# Process labels and create annotation entries\n",
        "annotation_id = 0\n",
        "for i, label in enumerate(labels):\n",
        "    bbox = [0, 0, image.shape[1], image.shape[0]]  # Assuming the whole image is the object\n",
        "    annotation_entry = {\n",
        "        \"id\": annotation_id,\n",
        "        \"image_id\": i,\n",
        "        \"category_id\": 1,  # Assuming all images belong to the \"face\" category\n",
        "        \"bbox\": bbox,\n",
        "        \"iscrowd\": 0,\n",
        "    }\n",
        "    coco_data[\"annotations\"].append(annotation_entry)\n",
        "    annotation_id += 1\n",
        "\n",
        "# Save COCO data to a JSON file\n",
        "with open(\"celeba_dataset.json\", \"w\") as f:\n",
        "    json.dump(coco_data, f)\n"
      ],
      "metadata": {
        "id": "MHR7YLO2-7_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The above code converts the dataset into coco format."
      ],
      "metadata": {
        "id": "I1l8DpdD_JLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this part of the assignment, the code accomplishes the following tasks:\n",
        "\n",
        "1. **Loading and Preprocessing Images**:\n",
        "   - It starts by loading images from the CelebA dataset using OpenCV's `cv2.imread()` function.\n",
        "   - Then, it converts the images from BGR format to RGB format using `cv2.cvtColor()` to maintain consistency with other libraries.\n",
        "   - Images are resized to 64x64 pixels to standardize their dimensions.\n",
        "   - Each processed image is appended to the `images` list, and the corresponding filename (without extension) is added to the `labels` list.\n",
        "\n",
        "2. **Normalization**:\n",
        "   - The loaded images are reshaped into a 2D array and normalized using `MinMaxScaler` from scikit-learn. This ensures that pixel values are scaled to the range [0, 1], which is a common preprocessing step in computer vision tasks.\n",
        "\n",
        "3. **Data Augmentation**:\n",
        "   - For data augmentation, the code performs two types of transformations: rotation and scaling.\n",
        "   - Rotated images are obtained by using OpenCV's `cv2.rotate()` function with a rotation angle of 90 degrees clockwise.\n",
        "   - Scaled images are generated using OpenCV's `cv2.resize()` function with a scaling factor of 1.2 in both dimensions.\n",
        "   - The augmented images are then added to the `augmented_images` list.\n",
        "\n",
        "4. **Dimensionality Reduction (PCA)**:\n",
        "   - PCA (Principal Component Analysis) is applied to the normalized images to reduce their dimensionality while preserving important information.\n",
        "   - The `PCA` class from scikit-learn is used for this purpose. We specify the number of components to keep (50 in this case) to reduce the dimensionality of the feature space.\n",
        "   - The transformed images are reshaped back to the original image dimensions after PCA.\n",
        "\n",
        "5. **Storing the Dataset**:\n",
        "   - Finally, all the processed data (original images, normalized images, augmented images, PCA-transformed images, and labels) are stored in an COCO file format using the `json` library. This format allows for efficient storage and retrieval of large numerical datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "9nUvq58Uz5P5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROBLEMS THAT WERE MET DURING IMPLEMENTATION OF THIS CODE SNIPPET IN THIS FIRST PART\n",
        "\n",
        "1. Finding a suitable dataset was time consuming. The process of looking for the suitable dataset with the required data was time consuming because of the wide variety of datasets out there.\n",
        "\n",
        "2. It took time to understand the library datasets' documentation. Some libraried were not easy to understand. For example, using the `tensorflow.keras.preprocessing.image` for augmentation was not easy to understand due to lack of experience in such a field of datasets for computer vision.\n",
        "\n",
        "3. We had to try using different export formats for our datasets because some were not working as intended such as when we used the `H5PY` library for computer vision dataset formats.\n"
      ],
      "metadata": {
        "id": "_ds6tmtZJ8Ls"
      }
    }
  ]
}